{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Run a batch data to get the loss value\n",
    "\n",
    "ignore the bert mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/esmfold/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json,time\n",
    "import customize_data\n",
    "import utils\n",
    "import numpy as np\n",
    "import esm.esmfold.v1.esmfold as ESM\n",
    "import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1cc1.L {'U'} SQAATPAADGKVKISIDPLTRVEGHLKIEVEVKDGKVVDAKCSGGMFRGFEQILRGRDPRDSSQIVQRICGVCPTAHCTASVMAQDDAFGVKVTTNGRITRNLIFGANYLQSHILHFYHLAALDYVKGPDVSPFVPRYANADLLTDRIKDGAKADATNTYGLNQYLKALEIRRICHEMVAMFGGRMPHVQGMVVGGATEIPTADKVAEYAARFKEVQKFVIEEYLPLIYTLGSVYTDLFETGIGWKNVIAFGVFPEDDDYKTFLLKPGVYIDGKDEEFDSKLVKEYVGHSFFDHSAPGGLHYSVGETNPNPDKPGAYSFVKAPRYKDKPCEVGPLARMWVQNPELSPVGQKLLKELYGIEAKKFRDLGDKAFSIMGRHVLRAEETWLTAVAVEKWLKQVQPGAETYVKSEIPDAAEGTGFTEAPRGALLHYLKIKDKKIENYQIVSATLWNANPRDDMGQRGPIEEALIGVPVPDIKNPVNVGRLVRSYDPULGCAVH\n",
      "1gp1.A {'U'} AAALAAAAPRTVYAFSARPLAGGEPFNLSSLRGKVLLIENVASLUGTTVRDYTQMNDLQRRLGPRGLVVLGFPCNQFGHQENAKNEEILNCLKYVRPGGGFEPNFMLFEKCEVNGEKAHPLFAFLREVLPTPSDDATALMTDPKFITWSPVCRNDVSWNFEKFLVGPDGVPVRRYSRRFLTIDIEPDIETLLSQGASA\n",
      "1nth.A {'O'} MTFRKSFDCYDFYDRAKVGEKCTQDDWDLMKIPMKAMELKQKYGLDFKGEFIPTDKDMMEKLFKAGFEMLLECGIYCTDTHRIVKYTEDEIWDAINNVQKEFVLGTGRDAVNVRKRSVGDKAKPIVQGGPTGSPISEDVFMPVHMSYALEKEVDTIVNGVMTSVRGKSPIPKSPYEVLAAKTETRLIKNACAMAGRPGMGVOGPETSLSAQGNISADCTGGMTCTDSHEVSQLNELKIDLDAISVIAHYKGNSDIIMDEQMPIFGGYAGGIEETTIVDVATHINAVLMSSASWHLDGPVHIRWGSTNTRETLMIAGWACATISEFTDILSGNQYYPCAGPCTEMCLLEASAQSITDTASGREILSGVASAKGVVTDKTTGMEARMMGEVARATAGVEISEVNVILDKLVSLYEKNYASAPAGKTFQECYDVKTVTPTEEYMQVYDGARKKLEDLGLVF\n",
      "2atc.A {'B'} ANPLYQSHIISINDLSRDDLNLVLATAAKLKANPQPELLKHKVIASCFFEASTRTRLSFQTSMHRLGASVVGFSDSANTSLGKKGQTLANTISVISTYVDAIVMRHPQEGAARLATEFSGNVPVLNAGDGSNQHPTQTLLDLFTIQQTEGRLNNLHVAMVGDLKYGRTVHSLTQALAKFDGNRFYFIAPDALAMPEYILDMLDEKGIAWSLHSSIEEVMTRVQKERLDPSEYABVKAQFLVRANSLGGLHNAKMNAKVLHPLPRVDEIATDVDKTPHAWYFQQAGNGIFARQALLALVLNRDLVL\n",
      "2xsk.A {'U'} MGSSQITFNTTQQGDMYTIIPEVTLTQSULURVQILSLREGSSGQSQTKQEKTLSLPANQPIALTKLSLNISPDDRVKIVVTVSDGQSLHLSQQWPPSSEKSLEHHHHHH\n",
      "bad seq:5\n",
      "too long:0\n"
     ]
    }
   ],
   "source": [
    "data_file = \"data/chain_set.jsonl\"\n",
    "dataset = customize_data.StructureDataset(data_file)\n",
    "print(f\"bad seq:{dataset.discard['bad_chars']}\\ntoo long:{dataset.discard['too_long']}\")\n",
    "dataLoader = customize_data.StructureDataloader(dataset,2,num_workers=1)\n",
    "batch_example = next(iter(dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['esm.embed_tokens.weight', 'esm.layers.0.self_attn.k_proj.weight', 'esm.layers.0.self_attn.k_proj.bias', 'esm.layers.0.self_attn.v_proj.weight', 'esm.layers.0.self_attn.v_proj.bias', 'esm.layers.0.self_attn.q_proj.weight', 'esm.layers.0.self_attn.q_proj.bias', 'esm.layers.0.self_attn.out_proj.weight', 'esm.layers.0.self_attn.out_proj.bias', 'esm.layers.0.self_attn.rot_emb.inv_freq', 'esm.layers.0.self_attn_layer_norm.weight', 'esm.layers.0.self_attn_layer_norm.bias', 'esm.layers.0.fc1.weight', 'esm.layers.0.fc1.bias', 'esm.layers.0.fc2.weight', 'esm.layers.0.fc2.bias', 'esm.layers.0.final_layer_norm.weight', 'esm.layers.0.final_layer_norm.bias', 'esm.layers.1.self_attn.k_proj.weight', 'esm.layers.1.self_attn.k_proj.bias', 'esm.layers.1.self_attn.v_proj.weight', 'esm.layers.1.self_attn.v_proj.bias', 'esm.layers.1.self_attn.q_proj.weight', 'esm.layers.1.self_attn.q_proj.bias', 'esm.layers.1.self_attn.out_proj.weight', 'esm.layers.1.self_attn.out_proj.bias', 'esm.layers.1.self_attn.rot_emb.inv_freq', 'esm.layers.1.self_attn_layer_norm.weight', 'esm.layers.1.self_attn_layer_norm.bias', 'esm.layers.1.fc1.weight', 'esm.layers.1.fc1.bias', 'esm.layers.1.fc2.weight', 'esm.layers.1.fc2.bias', 'esm.layers.1.final_layer_norm.weight', 'esm.layers.1.final_layer_norm.bias', 'esm.layers.2.self_attn.k_proj.weight', 'esm.layers.2.self_attn.k_proj.bias', 'esm.layers.2.self_attn.v_proj.weight', 'esm.layers.2.self_attn.v_proj.bias', 'esm.layers.2.self_attn.q_proj.weight', 'esm.layers.2.self_attn.q_proj.bias', 'esm.layers.2.self_attn.out_proj.weight', 'esm.layers.2.self_attn.out_proj.bias', 'esm.layers.2.self_attn.rot_emb.inv_freq', 'esm.layers.2.self_attn_layer_norm.weight', 'esm.layers.2.self_attn_layer_norm.bias', 'esm.layers.2.fc1.weight', 'esm.layers.2.fc1.bias', 'esm.layers.2.fc2.weight', 'esm.layers.2.fc2.bias', 'esm.layers.2.final_layer_norm.weight', 'esm.layers.2.final_layer_norm.bias', 'esm.layers.3.self_attn.k_proj.weight', 'esm.layers.3.self_attn.k_proj.bias', 'esm.layers.3.self_attn.v_proj.weight', 'esm.layers.3.self_attn.v_proj.bias', 'esm.layers.3.self_attn.q_proj.weight', 'esm.layers.3.self_attn.q_proj.bias', 'esm.layers.3.self_attn.out_proj.weight', 'esm.layers.3.self_attn.out_proj.bias', 'esm.layers.3.self_attn.rot_emb.inv_freq', 'esm.layers.3.self_attn_layer_norm.weight', 'esm.layers.3.self_attn_layer_norm.bias', 'esm.layers.3.fc1.weight', 'esm.layers.3.fc1.bias', 'esm.layers.3.fc2.weight', 'esm.layers.3.fc2.bias', 'esm.layers.3.final_layer_norm.weight', 'esm.layers.3.final_layer_norm.bias', 'esm.layers.4.self_attn.k_proj.weight', 'esm.layers.4.self_attn.k_proj.bias', 'esm.layers.4.self_attn.v_proj.weight', 'esm.layers.4.self_attn.v_proj.bias', 'esm.layers.4.self_attn.q_proj.weight', 'esm.layers.4.self_attn.q_proj.bias', 'esm.layers.4.self_attn.out_proj.weight', 'esm.layers.4.self_attn.out_proj.bias', 'esm.layers.4.self_attn.rot_emb.inv_freq', 'esm.layers.4.self_attn_layer_norm.weight', 'esm.layers.4.self_attn_layer_norm.bias', 'esm.layers.4.fc1.weight', 'esm.layers.4.fc1.bias', 'esm.layers.4.fc2.weight', 'esm.layers.4.fc2.bias', 'esm.layers.4.final_layer_norm.weight', 'esm.layers.4.final_layer_norm.bias', 'esm.layers.5.self_attn.k_proj.weight', 'esm.layers.5.self_attn.k_proj.bias', 'esm.layers.5.self_attn.v_proj.weight', 'esm.layers.5.self_attn.v_proj.bias', 'esm.layers.5.self_attn.q_proj.weight', 'esm.layers.5.self_attn.q_proj.bias', 'esm.layers.5.self_attn.out_proj.weight', 'esm.layers.5.self_attn.out_proj.bias', 'esm.layers.5.self_attn.rot_emb.inv_freq', 'esm.layers.5.self_attn_layer_norm.weight', 'esm.layers.5.self_attn_layer_norm.bias', 'esm.layers.5.fc1.weight', 'esm.layers.5.fc1.bias', 'esm.layers.5.fc2.weight', 'esm.layers.5.fc2.bias', 'esm.layers.5.final_layer_norm.weight', 'esm.layers.5.final_layer_norm.bias', 'esm.layers.6.self_attn.k_proj.weight', 'esm.layers.6.self_attn.k_proj.bias', 'esm.layers.6.self_attn.v_proj.weight', 'esm.layers.6.self_attn.v_proj.bias', 'esm.layers.6.self_attn.q_proj.weight', 'esm.layers.6.self_attn.q_proj.bias', 'esm.layers.6.self_attn.out_proj.weight', 'esm.layers.6.self_attn.out_proj.bias', 'esm.layers.6.self_attn.rot_emb.inv_freq', 'esm.layers.6.self_attn_layer_norm.weight', 'esm.layers.6.self_attn_layer_norm.bias', 'esm.layers.6.fc1.weight', 'esm.layers.6.fc1.bias', 'esm.layers.6.fc2.weight', 'esm.layers.6.fc2.bias', 'esm.layers.6.final_layer_norm.weight', 'esm.layers.6.final_layer_norm.bias', 'esm.layers.7.self_attn.k_proj.weight', 'esm.layers.7.self_attn.k_proj.bias', 'esm.layers.7.self_attn.v_proj.weight', 'esm.layers.7.self_attn.v_proj.bias', 'esm.layers.7.self_attn.q_proj.weight', 'esm.layers.7.self_attn.q_proj.bias', 'esm.layers.7.self_attn.out_proj.weight', 'esm.layers.7.self_attn.out_proj.bias', 'esm.layers.7.self_attn.rot_emb.inv_freq', 'esm.layers.7.self_attn_layer_norm.weight', 'esm.layers.7.self_attn_layer_norm.bias', 'esm.layers.7.fc1.weight', 'esm.layers.7.fc1.bias', 'esm.layers.7.fc2.weight', 'esm.layers.7.fc2.bias', 'esm.layers.7.final_layer_norm.weight', 'esm.layers.7.final_layer_norm.bias', 'esm.layers.8.self_attn.k_proj.weight', 'esm.layers.8.self_attn.k_proj.bias', 'esm.layers.8.self_attn.v_proj.weight', 'esm.layers.8.self_attn.v_proj.bias', 'esm.layers.8.self_attn.q_proj.weight', 'esm.layers.8.self_attn.q_proj.bias', 'esm.layers.8.self_attn.out_proj.weight', 'esm.layers.8.self_attn.out_proj.bias', 'esm.layers.8.self_attn.rot_emb.inv_freq', 'esm.layers.8.self_attn_layer_norm.weight', 'esm.layers.8.self_attn_layer_norm.bias', 'esm.layers.8.fc1.weight', 'esm.layers.8.fc1.bias', 'esm.layers.8.fc2.weight', 'esm.layers.8.fc2.bias', 'esm.layers.8.final_layer_norm.weight', 'esm.layers.8.final_layer_norm.bias', 'esm.layers.9.self_attn.k_proj.weight', 'esm.layers.9.self_attn.k_proj.bias', 'esm.layers.9.self_attn.v_proj.weight', 'esm.layers.9.self_attn.v_proj.bias', 'esm.layers.9.self_attn.q_proj.weight', 'esm.layers.9.self_attn.q_proj.bias', 'esm.layers.9.self_attn.out_proj.weight', 'esm.layers.9.self_attn.out_proj.bias', 'esm.layers.9.self_attn.rot_emb.inv_freq', 'esm.layers.9.self_attn_layer_norm.weight', 'esm.layers.9.self_attn_layer_norm.bias', 'esm.layers.9.fc1.weight', 'esm.layers.9.fc1.bias', 'esm.layers.9.fc2.weight', 'esm.layers.9.fc2.bias', 'esm.layers.9.final_layer_norm.weight', 'esm.layers.9.final_layer_norm.bias', 'esm.layers.10.self_attn.k_proj.weight', 'esm.layers.10.self_attn.k_proj.bias', 'esm.layers.10.self_attn.v_proj.weight', 'esm.layers.10.self_attn.v_proj.bias', 'esm.layers.10.self_attn.q_proj.weight', 'esm.layers.10.self_attn.q_proj.bias', 'esm.layers.10.self_attn.out_proj.weight', 'esm.layers.10.self_attn.out_proj.bias', 'esm.layers.10.self_attn.rot_emb.inv_freq', 'esm.layers.10.self_attn_layer_norm.weight', 'esm.layers.10.self_attn_layer_norm.bias', 'esm.layers.10.fc1.weight', 'esm.layers.10.fc1.bias', 'esm.layers.10.fc2.weight', 'esm.layers.10.fc2.bias', 'esm.layers.10.final_layer_norm.weight', 'esm.layers.10.final_layer_norm.bias', 'esm.layers.11.self_attn.k_proj.weight', 'esm.layers.11.self_attn.k_proj.bias', 'esm.layers.11.self_attn.v_proj.weight', 'esm.layers.11.self_attn.v_proj.bias', 'esm.layers.11.self_attn.q_proj.weight', 'esm.layers.11.self_attn.q_proj.bias', 'esm.layers.11.self_attn.out_proj.weight', 'esm.layers.11.self_attn.out_proj.bias', 'esm.layers.11.self_attn.rot_emb.inv_freq', 'esm.layers.11.self_attn_layer_norm.weight', 'esm.layers.11.self_attn_layer_norm.bias', 'esm.layers.11.fc1.weight', 'esm.layers.11.fc1.bias', 'esm.layers.11.fc2.weight', 'esm.layers.11.fc2.bias', 'esm.layers.11.final_layer_norm.weight', 'esm.layers.11.final_layer_norm.bias', 'esm.layers.12.self_attn.k_proj.weight', 'esm.layers.12.self_attn.k_proj.bias', 'esm.layers.12.self_attn.v_proj.weight', 'esm.layers.12.self_attn.v_proj.bias', 'esm.layers.12.self_attn.q_proj.weight', 'esm.layers.12.self_attn.q_proj.bias', 'esm.layers.12.self_attn.out_proj.weight', 'esm.layers.12.self_attn.out_proj.bias', 'esm.layers.12.self_attn.rot_emb.inv_freq', 'esm.layers.12.self_attn_layer_norm.weight', 'esm.layers.12.self_attn_layer_norm.bias', 'esm.layers.12.fc1.weight', 'esm.layers.12.fc1.bias', 'esm.layers.12.fc2.weight', 'esm.layers.12.fc2.bias', 'esm.layers.12.final_layer_norm.weight', 'esm.layers.12.final_layer_norm.bias', 'esm.layers.13.self_attn.k_proj.weight', 'esm.layers.13.self_attn.k_proj.bias', 'esm.layers.13.self_attn.v_proj.weight', 'esm.layers.13.self_attn.v_proj.bias', 'esm.layers.13.self_attn.q_proj.weight', 'esm.layers.13.self_attn.q_proj.bias', 'esm.layers.13.self_attn.out_proj.weight', 'esm.layers.13.self_attn.out_proj.bias', 'esm.layers.13.self_attn.rot_emb.inv_freq', 'esm.layers.13.self_attn_layer_norm.weight', 'esm.layers.13.self_attn_layer_norm.bias', 'esm.layers.13.fc1.weight', 'esm.layers.13.fc1.bias', 'esm.layers.13.fc2.weight', 'esm.layers.13.fc2.bias', 'esm.layers.13.final_layer_norm.weight', 'esm.layers.13.final_layer_norm.bias', 'esm.layers.14.self_attn.k_proj.weight', 'esm.layers.14.self_attn.k_proj.bias', 'esm.layers.14.self_attn.v_proj.weight', 'esm.layers.14.self_attn.v_proj.bias', 'esm.layers.14.self_attn.q_proj.weight', 'esm.layers.14.self_attn.q_proj.bias', 'esm.layers.14.self_attn.out_proj.weight', 'esm.layers.14.self_attn.out_proj.bias', 'esm.layers.14.self_attn.rot_emb.inv_freq', 'esm.layers.14.self_attn_layer_norm.weight', 'esm.layers.14.self_attn_layer_norm.bias', 'esm.layers.14.fc1.weight', 'esm.layers.14.fc1.bias', 'esm.layers.14.fc2.weight', 'esm.layers.14.fc2.bias', 'esm.layers.14.final_layer_norm.weight', 'esm.layers.14.final_layer_norm.bias', 'esm.layers.15.self_attn.k_proj.weight', 'esm.layers.15.self_attn.k_proj.bias', 'esm.layers.15.self_attn.v_proj.weight', 'esm.layers.15.self_attn.v_proj.bias', 'esm.layers.15.self_attn.q_proj.weight', 'esm.layers.15.self_attn.q_proj.bias', 'esm.layers.15.self_attn.out_proj.weight', 'esm.layers.15.self_attn.out_proj.bias', 'esm.layers.15.self_attn.rot_emb.inv_freq', 'esm.layers.15.self_attn_layer_norm.weight', 'esm.layers.15.self_attn_layer_norm.bias', 'esm.layers.15.fc1.weight', 'esm.layers.15.fc1.bias', 'esm.layers.15.fc2.weight', 'esm.layers.15.fc2.bias', 'esm.layers.15.final_layer_norm.weight', 'esm.layers.15.final_layer_norm.bias', 'esm.layers.16.self_attn.k_proj.weight', 'esm.layers.16.self_attn.k_proj.bias', 'esm.layers.16.self_attn.v_proj.weight', 'esm.layers.16.self_attn.v_proj.bias', 'esm.layers.16.self_attn.q_proj.weight', 'esm.layers.16.self_attn.q_proj.bias', 'esm.layers.16.self_attn.out_proj.weight', 'esm.layers.16.self_attn.out_proj.bias', 'esm.layers.16.self_attn.rot_emb.inv_freq', 'esm.layers.16.self_attn_layer_norm.weight', 'esm.layers.16.self_attn_layer_norm.bias', 'esm.layers.16.fc1.weight', 'esm.layers.16.fc1.bias', 'esm.layers.16.fc2.weight', 'esm.layers.16.fc2.bias', 'esm.layers.16.final_layer_norm.weight', 'esm.layers.16.final_layer_norm.bias', 'esm.layers.17.self_attn.k_proj.weight', 'esm.layers.17.self_attn.k_proj.bias', 'esm.layers.17.self_attn.v_proj.weight', 'esm.layers.17.self_attn.v_proj.bias', 'esm.layers.17.self_attn.q_proj.weight', 'esm.layers.17.self_attn.q_proj.bias', 'esm.layers.17.self_attn.out_proj.weight', 'esm.layers.17.self_attn.out_proj.bias', 'esm.layers.17.self_attn.rot_emb.inv_freq', 'esm.layers.17.self_attn_layer_norm.weight', 'esm.layers.17.self_attn_layer_norm.bias', 'esm.layers.17.fc1.weight', 'esm.layers.17.fc1.bias', 'esm.layers.17.fc2.weight', 'esm.layers.17.fc2.bias', 'esm.layers.17.final_layer_norm.weight', 'esm.layers.17.final_layer_norm.bias', 'esm.layers.18.self_attn.k_proj.weight', 'esm.layers.18.self_attn.k_proj.bias', 'esm.layers.18.self_attn.v_proj.weight', 'esm.layers.18.self_attn.v_proj.bias', 'esm.layers.18.self_attn.q_proj.weight', 'esm.layers.18.self_attn.q_proj.bias', 'esm.layers.18.self_attn.out_proj.weight', 'esm.layers.18.self_attn.out_proj.bias', 'esm.layers.18.self_attn.rot_emb.inv_freq', 'esm.layers.18.self_attn_layer_norm.weight', 'esm.layers.18.self_attn_layer_norm.bias', 'esm.layers.18.fc1.weight', 'esm.layers.18.fc1.bias', 'esm.layers.18.fc2.weight', 'esm.layers.18.fc2.bias', 'esm.layers.18.final_layer_norm.weight', 'esm.layers.18.final_layer_norm.bias', 'esm.layers.19.self_attn.k_proj.weight', 'esm.layers.19.self_attn.k_proj.bias', 'esm.layers.19.self_attn.v_proj.weight', 'esm.layers.19.self_attn.v_proj.bias', 'esm.layers.19.self_attn.q_proj.weight', 'esm.layers.19.self_attn.q_proj.bias', 'esm.layers.19.self_attn.out_proj.weight', 'esm.layers.19.self_attn.out_proj.bias', 'esm.layers.19.self_attn.rot_emb.inv_freq', 'esm.layers.19.self_attn_layer_norm.weight', 'esm.layers.19.self_attn_layer_norm.bias', 'esm.layers.19.fc1.weight', 'esm.layers.19.fc1.bias', 'esm.layers.19.fc2.weight', 'esm.layers.19.fc2.bias', 'esm.layers.19.final_layer_norm.weight', 'esm.layers.19.final_layer_norm.bias', 'esm.layers.20.self_attn.k_proj.weight', 'esm.layers.20.self_attn.k_proj.bias', 'esm.layers.20.self_attn.v_proj.weight', 'esm.layers.20.self_attn.v_proj.bias', 'esm.layers.20.self_attn.q_proj.weight', 'esm.layers.20.self_attn.q_proj.bias', 'esm.layers.20.self_attn.out_proj.weight', 'esm.layers.20.self_attn.out_proj.bias', 'esm.layers.20.self_attn.rot_emb.inv_freq', 'esm.layers.20.self_attn_layer_norm.weight', 'esm.layers.20.self_attn_layer_norm.bias', 'esm.layers.20.fc1.weight', 'esm.layers.20.fc1.bias', 'esm.layers.20.fc2.weight', 'esm.layers.20.fc2.bias', 'esm.layers.20.final_layer_norm.weight', 'esm.layers.20.final_layer_norm.bias', 'esm.layers.21.self_attn.k_proj.weight', 'esm.layers.21.self_attn.k_proj.bias', 'esm.layers.21.self_attn.v_proj.weight', 'esm.layers.21.self_attn.v_proj.bias', 'esm.layers.21.self_attn.q_proj.weight', 'esm.layers.21.self_attn.q_proj.bias', 'esm.layers.21.self_attn.out_proj.weight', 'esm.layers.21.self_attn.out_proj.bias', 'esm.layers.21.self_attn.rot_emb.inv_freq', 'esm.layers.21.self_attn_layer_norm.weight', 'esm.layers.21.self_attn_layer_norm.bias', 'esm.layers.21.fc1.weight', 'esm.layers.21.fc1.bias', 'esm.layers.21.fc2.weight', 'esm.layers.21.fc2.bias', 'esm.layers.21.final_layer_norm.weight', 'esm.layers.21.final_layer_norm.bias', 'esm.layers.22.self_attn.k_proj.weight', 'esm.layers.22.self_attn.k_proj.bias', 'esm.layers.22.self_attn.v_proj.weight', 'esm.layers.22.self_attn.v_proj.bias', 'esm.layers.22.self_attn.q_proj.weight', 'esm.layers.22.self_attn.q_proj.bias', 'esm.layers.22.self_attn.out_proj.weight', 'esm.layers.22.self_attn.out_proj.bias', 'esm.layers.22.self_attn.rot_emb.inv_freq', 'esm.layers.22.self_attn_layer_norm.weight', 'esm.layers.22.self_attn_layer_norm.bias', 'esm.layers.22.fc1.weight', 'esm.layers.22.fc1.bias', 'esm.layers.22.fc2.weight', 'esm.layers.22.fc2.bias', 'esm.layers.22.final_layer_norm.weight', 'esm.layers.22.final_layer_norm.bias', 'esm.layers.23.self_attn.k_proj.weight', 'esm.layers.23.self_attn.k_proj.bias', 'esm.layers.23.self_attn.v_proj.weight', 'esm.layers.23.self_attn.v_proj.bias', 'esm.layers.23.self_attn.q_proj.weight', 'esm.layers.23.self_attn.q_proj.bias', 'esm.layers.23.self_attn.out_proj.weight', 'esm.layers.23.self_attn.out_proj.bias', 'esm.layers.23.self_attn.rot_emb.inv_freq', 'esm.layers.23.self_attn_layer_norm.weight', 'esm.layers.23.self_attn_layer_norm.bias', 'esm.layers.23.fc1.weight', 'esm.layers.23.fc1.bias', 'esm.layers.23.fc2.weight', 'esm.layers.23.fc2.bias', 'esm.layers.23.final_layer_norm.weight', 'esm.layers.23.final_layer_norm.bias', 'esm.layers.24.self_attn.k_proj.weight', 'esm.layers.24.self_attn.k_proj.bias', 'esm.layers.24.self_attn.v_proj.weight', 'esm.layers.24.self_attn.v_proj.bias', 'esm.layers.24.self_attn.q_proj.weight', 'esm.layers.24.self_attn.q_proj.bias', 'esm.layers.24.self_attn.out_proj.weight', 'esm.layers.24.self_attn.out_proj.bias', 'esm.layers.24.self_attn.rot_emb.inv_freq', 'esm.layers.24.self_attn_layer_norm.weight', 'esm.layers.24.self_attn_layer_norm.bias', 'esm.layers.24.fc1.weight', 'esm.layers.24.fc1.bias', 'esm.layers.24.fc2.weight', 'esm.layers.24.fc2.bias', 'esm.layers.24.final_layer_norm.weight', 'esm.layers.24.final_layer_norm.bias', 'esm.layers.25.self_attn.k_proj.weight', 'esm.layers.25.self_attn.k_proj.bias', 'esm.layers.25.self_attn.v_proj.weight', 'esm.layers.25.self_attn.v_proj.bias', 'esm.layers.25.self_attn.q_proj.weight', 'esm.layers.25.self_attn.q_proj.bias', 'esm.layers.25.self_attn.out_proj.weight', 'esm.layers.25.self_attn.out_proj.bias', 'esm.layers.25.self_attn.rot_emb.inv_freq', 'esm.layers.25.self_attn_layer_norm.weight', 'esm.layers.25.self_attn_layer_norm.bias', 'esm.layers.25.fc1.weight', 'esm.layers.25.fc1.bias', 'esm.layers.25.fc2.weight', 'esm.layers.25.fc2.bias', 'esm.layers.25.final_layer_norm.weight', 'esm.layers.25.final_layer_norm.bias', 'esm.layers.26.self_attn.k_proj.weight', 'esm.layers.26.self_attn.k_proj.bias', 'esm.layers.26.self_attn.v_proj.weight', 'esm.layers.26.self_attn.v_proj.bias', 'esm.layers.26.self_attn.q_proj.weight', 'esm.layers.26.self_attn.q_proj.bias', 'esm.layers.26.self_attn.out_proj.weight', 'esm.layers.26.self_attn.out_proj.bias', 'esm.layers.26.self_attn.rot_emb.inv_freq', 'esm.layers.26.self_attn_layer_norm.weight', 'esm.layers.26.self_attn_layer_norm.bias', 'esm.layers.26.fc1.weight', 'esm.layers.26.fc1.bias', 'esm.layers.26.fc2.weight', 'esm.layers.26.fc2.bias', 'esm.layers.26.final_layer_norm.weight', 'esm.layers.26.final_layer_norm.bias', 'esm.layers.27.self_attn.k_proj.weight', 'esm.layers.27.self_attn.k_proj.bias', 'esm.layers.27.self_attn.v_proj.weight', 'esm.layers.27.self_attn.v_proj.bias', 'esm.layers.27.self_attn.q_proj.weight', 'esm.layers.27.self_attn.q_proj.bias', 'esm.layers.27.self_attn.out_proj.weight', 'esm.layers.27.self_attn.out_proj.bias', 'esm.layers.27.self_attn.rot_emb.inv_freq', 'esm.layers.27.self_attn_layer_norm.weight', 'esm.layers.27.self_attn_layer_norm.bias', 'esm.layers.27.fc1.weight', 'esm.layers.27.fc1.bias', 'esm.layers.27.fc2.weight', 'esm.layers.27.fc2.bias', 'esm.layers.27.final_layer_norm.weight', 'esm.layers.27.final_layer_norm.bias', 'esm.layers.28.self_attn.k_proj.weight', 'esm.layers.28.self_attn.k_proj.bias', 'esm.layers.28.self_attn.v_proj.weight', 'esm.layers.28.self_attn.v_proj.bias', 'esm.layers.28.self_attn.q_proj.weight', 'esm.layers.28.self_attn.q_proj.bias', 'esm.layers.28.self_attn.out_proj.weight', 'esm.layers.28.self_attn.out_proj.bias', 'esm.layers.28.self_attn.rot_emb.inv_freq', 'esm.layers.28.self_attn_layer_norm.weight', 'esm.layers.28.self_attn_layer_norm.bias', 'esm.layers.28.fc1.weight', 'esm.layers.28.fc1.bias', 'esm.layers.28.fc2.weight', 'esm.layers.28.fc2.bias', 'esm.layers.28.final_layer_norm.weight', 'esm.layers.28.final_layer_norm.bias', 'esm.layers.29.self_attn.k_proj.weight', 'esm.layers.29.self_attn.k_proj.bias', 'esm.layers.29.self_attn.v_proj.weight', 'esm.layers.29.self_attn.v_proj.bias', 'esm.layers.29.self_attn.q_proj.weight', 'esm.layers.29.self_attn.q_proj.bias', 'esm.layers.29.self_attn.out_proj.weight', 'esm.layers.29.self_attn.out_proj.bias', 'esm.layers.29.self_attn.rot_emb.inv_freq', 'esm.layers.29.self_attn_layer_norm.weight', 'esm.layers.29.self_attn_layer_norm.bias', 'esm.layers.29.fc1.weight', 'esm.layers.29.fc1.bias', 'esm.layers.29.fc2.weight', 'esm.layers.29.fc2.bias', 'esm.layers.29.final_layer_norm.weight', 'esm.layers.29.final_layer_norm.bias', 'esm.layers.30.self_attn.k_proj.weight', 'esm.layers.30.self_attn.k_proj.bias', 'esm.layers.30.self_attn.v_proj.weight', 'esm.layers.30.self_attn.v_proj.bias', 'esm.layers.30.self_attn.q_proj.weight', 'esm.layers.30.self_attn.q_proj.bias', 'esm.layers.30.self_attn.out_proj.weight', 'esm.layers.30.self_attn.out_proj.bias', 'esm.layers.30.self_attn.rot_emb.inv_freq', 'esm.layers.30.self_attn_layer_norm.weight', 'esm.layers.30.self_attn_layer_norm.bias', 'esm.layers.30.fc1.weight', 'esm.layers.30.fc1.bias', 'esm.layers.30.fc2.weight', 'esm.layers.30.fc2.bias', 'esm.layers.30.final_layer_norm.weight', 'esm.layers.30.final_layer_norm.bias', 'esm.layers.31.self_attn.k_proj.weight', 'esm.layers.31.self_attn.k_proj.bias', 'esm.layers.31.self_attn.v_proj.weight', 'esm.layers.31.self_attn.v_proj.bias', 'esm.layers.31.self_attn.q_proj.weight', 'esm.layers.31.self_attn.q_proj.bias', 'esm.layers.31.self_attn.out_proj.weight', 'esm.layers.31.self_attn.out_proj.bias', 'esm.layers.31.self_attn.rot_emb.inv_freq', 'esm.layers.31.self_attn_layer_norm.weight', 'esm.layers.31.self_attn_layer_norm.bias', 'esm.layers.31.fc1.weight', 'esm.layers.31.fc1.bias', 'esm.layers.31.fc2.weight', 'esm.layers.31.fc2.bias', 'esm.layers.31.final_layer_norm.weight', 'esm.layers.31.final_layer_norm.bias', 'esm.layers.32.self_attn.k_proj.weight', 'esm.layers.32.self_attn.k_proj.bias', 'esm.layers.32.self_attn.v_proj.weight', 'esm.layers.32.self_attn.v_proj.bias', 'esm.layers.32.self_attn.q_proj.weight', 'esm.layers.32.self_attn.q_proj.bias', 'esm.layers.32.self_attn.out_proj.weight', 'esm.layers.32.self_attn.out_proj.bias', 'esm.layers.32.self_attn.rot_emb.inv_freq', 'esm.layers.32.self_attn_layer_norm.weight', 'esm.layers.32.self_attn_layer_norm.bias', 'esm.layers.32.fc1.weight', 'esm.layers.32.fc1.bias', 'esm.layers.32.fc2.weight', 'esm.layers.32.fc2.bias', 'esm.layers.32.final_layer_norm.weight', 'esm.layers.32.final_layer_norm.bias', 'esm.layers.33.self_attn.k_proj.weight', 'esm.layers.33.self_attn.k_proj.bias', 'esm.layers.33.self_attn.v_proj.weight', 'esm.layers.33.self_attn.v_proj.bias', 'esm.layers.33.self_attn.q_proj.weight', 'esm.layers.33.self_attn.q_proj.bias', 'esm.layers.33.self_attn.out_proj.weight', 'esm.layers.33.self_attn.out_proj.bias', 'esm.layers.33.self_attn.rot_emb.inv_freq', 'esm.layers.33.self_attn_layer_norm.weight', 'esm.layers.33.self_attn_layer_norm.bias', 'esm.layers.33.fc1.weight', 'esm.layers.33.fc1.bias', 'esm.layers.33.fc2.weight', 'esm.layers.33.fc2.bias', 'esm.layers.33.final_layer_norm.weight', 'esm.layers.33.final_layer_norm.bias', 'esm.layers.34.self_attn.k_proj.weight', 'esm.layers.34.self_attn.k_proj.bias', 'esm.layers.34.self_attn.v_proj.weight', 'esm.layers.34.self_attn.v_proj.bias', 'esm.layers.34.self_attn.q_proj.weight', 'esm.layers.34.self_attn.q_proj.bias', 'esm.layers.34.self_attn.out_proj.weight', 'esm.layers.34.self_attn.out_proj.bias', 'esm.layers.34.self_attn.rot_emb.inv_freq', 'esm.layers.34.self_attn_layer_norm.weight', 'esm.layers.34.self_attn_layer_norm.bias', 'esm.layers.34.fc1.weight', 'esm.layers.34.fc1.bias', 'esm.layers.34.fc2.weight', 'esm.layers.34.fc2.bias', 'esm.layers.34.final_layer_norm.weight', 'esm.layers.34.final_layer_norm.bias', 'esm.layers.35.self_attn.k_proj.weight', 'esm.layers.35.self_attn.k_proj.bias', 'esm.layers.35.self_attn.v_proj.weight', 'esm.layers.35.self_attn.v_proj.bias', 'esm.layers.35.self_attn.q_proj.weight', 'esm.layers.35.self_attn.q_proj.bias', 'esm.layers.35.self_attn.out_proj.weight', 'esm.layers.35.self_attn.out_proj.bias', 'esm.layers.35.self_attn.rot_emb.inv_freq', 'esm.layers.35.self_attn_layer_norm.weight', 'esm.layers.35.self_attn_layer_norm.bias', 'esm.layers.35.fc1.weight', 'esm.layers.35.fc1.bias', 'esm.layers.35.fc2.weight', 'esm.layers.35.fc2.bias', 'esm.layers.35.final_layer_norm.weight', 'esm.layers.35.final_layer_norm.bias', 'esm.contact_head.regression.weight', 'esm.contact_head.regression.bias', 'esm.emb_layer_norm_after.weight', 'esm.emb_layer_norm_after.bias', 'esm.lm_head.weight', 'esm.lm_head.bias', 'esm.lm_head.dense.weight', 'esm.lm_head.dense.bias', 'esm.lm_head.layer_norm.weight', 'esm.lm_head.layer_norm.bias'], unexpected_keys=['positional_encoding._float_tensor', 'trunk.structure_module.default_frames', 'trunk.structure_module.group_idx', 'trunk.structure_module.atom_mask', 'trunk.structure_module.lit_positions'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data file and initialize the esm-inpainting class\n",
    "model_path = \"/root/.cache/torch/hub/checkpoints/esmfold_3B_v1.pt\"\n",
    "model_data = torch.load(str(model_path), map_location=\"cuda:0\") #读取一个pickle文件为一个dict\n",
    "cfg = model_data[\"cfg\"][\"model\"]\n",
    "model = modules.esm_inpaint(cfg) # make an instance\n",
    "model_state = model_data[\"model\"]\n",
    "model.esmfold.load_state_dict(model_state, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "# 11735MiB / 32510MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         dis_embed: torch.Tensor,\n",
    "#         bb_frame: torch.Tensor,\n",
    "#         aa: torch.Tensor,\n",
    "#         mask: T.Optional[torch.Tensor] = None,\n",
    "#         residx: T.Optional[torch.Tensor] = None,\n",
    "#         masking_pattern: T.Optional[torch.Tensor] = None,\n",
    "#         num_recycles: T.Optional[int] = None,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': 'torch.Size([2, 379, 4, 3])_torch.float32',\n",
       " 'seq': 'torch.Size([2, 379])_torch.int64',\n",
       " 'bert_mask_fraction': 'torch.Size([2, 1])_torch.float32',\n",
       " 'bert_mask': 'torch.Size([2, 379])_torch.float32',\n",
       " 'padding_mask': 'torch.Size([2, 379])_torch.float32'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.recur_print(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "coord cuda:0\n",
      " mask cuda:0\n",
      " mask_frame cuda:0\n",
      "embedding cuda:0\n",
      " bb_frame cuda:0\n",
      "torch.Size([2, 379, 379, 128])\n",
      "Recycling:0-----------\n",
      "Folding Trunk\n",
      "Structure Module\n",
      "ipa module 0\n",
      "ipa dropout\n",
      "ipa layernormalization\n",
      "transition\n",
      "update backbone frames\n",
      "All side chain frames\n",
      "xyz prediction\n",
      "ipa module 1\n",
      "ipa dropout\n",
      "ipa layernormalization\n",
      "transition\n",
      "update backbone frames\n",
      "All side chain frames\n",
      "xyz prediction\n",
      "ipa module 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m batch_example\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     batch_example[key] \u001b[39m=\u001b[39m batch_example[key]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m output \u001b[39m=\u001b[39m model(coord\u001b[39m=\u001b[39;49mbatch_example[\u001b[39m'\u001b[39;49m\u001b[39mcoord\u001b[39;49m\u001b[39m'\u001b[39;49m],mask\u001b[39m=\u001b[39;49m(batch_example[\u001b[39m'\u001b[39;49m\u001b[39mpadding_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32),S\u001b[39m=\u001b[39;49mbatch_example[\u001b[39m'\u001b[39;49m\u001b[39mseq\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/conda/envs/esmfold/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ESM-Inpainting/esm_inpaint/modules.py:81\u001b[0m, in \u001b[0;36mesm_inpaint.forward\u001b[0;34m(self, coord, mask, S)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoord \u001b[39m\u001b[39m{\u001b[39;00mcoord\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m mask \u001b[39m\u001b[39m{\u001b[39;00mcoord\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m mask_frame \u001b[39m\u001b[39m{\u001b[39;00mmask\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39membedding \u001b[39m\u001b[39m{\u001b[39;00mdis_embed\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m bb_frame \u001b[39m\u001b[39m{\u001b[39;00mbb_frame\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[39mprint\u001b[39m(dis_embed\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 81\u001b[0m structure \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mesmfold(dis_embed,bb_frame,S,mask)\n\u001b[1;32m     82\u001b[0m \u001b[39mreturn\u001b[39;00m structure\n",
      "File \u001b[0;32m/opt/conda/envs/esmfold/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ESM-Inpainting/esm_inpaint/esm/esmfold/v1/esmfold.py:178\u001b[0m, in \u001b[0;36mESMFold.forward\u001b[0;34m(self, dis_embed, bb_frame, aa, mask, residx, masking_pattern, num_recycles)\u001b[0m\n\u001b[1;32m    174\u001b[0m s_z_0 \u001b[39m=\u001b[39m s_s_0\u001b[39m.\u001b[39mnew_zeros(B, L, L, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mtrunk\u001b[39m.\u001b[39mpairwise_state_dim)\n\u001b[1;32m    176\u001b[0m s_s_0 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(aa)\n\u001b[0;32m--> 178\u001b[0m structure: \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrunk(\n\u001b[1;32m    179\u001b[0m     dis_embed, bb_frame, s_s_0, s_z_0, aa, residx, mask, no_recycles\u001b[39m=\u001b[39;49mnum_recycles)\n\u001b[1;32m    180\u001b[0m \u001b[39m# Documenting what we expect:\u001b[39;00m\n\u001b[1;32m    181\u001b[0m structure \u001b[39m=\u001b[39m {\n\u001b[1;32m    182\u001b[0m     k: v\n\u001b[1;32m    183\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m structure\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     ]\n\u001b[1;32m    195\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/envs/esmfold/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ESM-Inpainting/esm_inpaint/esm/esmfold/v1/trunk.py:216\u001b[0m, in \u001b[0;36mFoldingTrunk.forward\u001b[0;34m(self, dis_embed, bb_frame, seq_feats, pair_feats, true_aa, residx, mask, no_recycles)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStructure Module\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m s_z \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dis_embed\n\u001b[0;32m--> 216\u001b[0m structure \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructure_module(\n\u001b[1;32m    217\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39msingle\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrunk2sm_s(\n\u001b[1;32m    218\u001b[0m         s_s), \u001b[39m\"\u001b[39;49m\u001b[39mpair\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrunk2sm_z(s_z)},\n\u001b[1;32m    219\u001b[0m     true_aa,\n\u001b[1;32m    220\u001b[0m     mask\u001b[39m.\u001b[39;49mfloat(),\n\u001b[1;32m    221\u001b[0m     initial_bb_frame \u001b[39m=\u001b[39;49m bb_frame\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRecycling Output\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m recycle_s \u001b[39m=\u001b[39m s_s\n",
      "File \u001b[0;32m/opt/conda/envs/esmfold/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/esmfold/lib/python3.9/site-packages/openfold/model/structure_module.py:678\u001b[0m, in \u001b[0;36mStructureModule.forward\u001b[0;34m(self, evoformer_output_dict, aatype, mask, inplace_safe, _offload_inference, initial_bb_frame)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_blocks):\n\u001b[1;32m    676\u001b[0m     \u001b[39m# [*, N, C_s]\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mipa module \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 678\u001b[0m     s \u001b[39m=\u001b[39m s \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mipa(\n\u001b[1;32m    679\u001b[0m         s, \n\u001b[1;32m    680\u001b[0m         z, \n\u001b[1;32m    681\u001b[0m         rigids, \n\u001b[1;32m    682\u001b[0m         mask, \n\u001b[1;32m    683\u001b[0m         inplace_safe\u001b[39m=\u001b[39;49minplace_safe,\n\u001b[1;32m    684\u001b[0m         _offload_inference\u001b[39m=\u001b[39;49m_offload_inference, \n\u001b[1;32m    685\u001b[0m         _z_reference_list\u001b[39m=\u001b[39;49mz_reference_list\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mipa dropout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    688\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mipa_dropout(s)\n",
      "File \u001b[0;32m/opt/conda/envs/esmfold/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/esmfold/lib/python3.9/site-packages/openfold/model/structure_module.py:368\u001b[0m, in \u001b[0;36mInvariantPointAttention.forward\u001b[0;34m(self, s, z, r, mask, inplace_safe, _offload_inference, _z_reference_list)\u001b[0m\n\u001b[1;32m    362\u001b[0m     a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(a)\n\u001b[1;32m    364\u001b[0m \u001b[39m################\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m# Compute output\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m################\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# [*, N_res, H, C_hidden]\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m o \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(\n\u001b[1;32m    369\u001b[0m     a, v\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49ma\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    370\u001b[0m )\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m    372\u001b[0m \u001b[39m# [*, N_res, H * C_hidden]\u001b[39;00m\n\u001b[1;32m    373\u001b[0m o \u001b[39m=\u001b[39m flatten_final_dims(o, \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for key in batch_example.keys():\n",
    "        batch_example[key] = batch_example[key].to(\"cuda\")\n",
    "    output = model(coord=batch_example['coord'],mask=(batch_example['padding_mask']).to(torch.float32),S=batch_example['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 190])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_example['padding_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.7'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59ac7c7a591b3b1c7843a2ac3c1da3475c2ffdc4684f266052d1e649f1d10700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

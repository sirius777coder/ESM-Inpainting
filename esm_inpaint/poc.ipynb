{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Run a batch data to get the loss value\n",
    "\n",
    "ignore the bert mask\n",
    "\n",
    "记录问题openfold/feats.py/torsion_angles_to_frames/default_4x4 = rrgdf[aatype, ...]报错\n",
    "\n",
    "一个经典的问题，如何用一个tensor对另一个tensor切片----比较灵活的常量高维embedding\n",
    "\n",
    "我定义的dataset : 0-19 + X + padding 一共22个token，但是只定义了21种frame constant\n",
    "\n",
    "target_tensor[query_tensor] 进行检索时，实际是用query_sequence的每一个值去检索target_tensor的dim0 对应的向量\n",
    "\n",
    "返回的shape是 (*target_tensor.shape,*query_tensor.shape[:1])\n",
    "\n",
    "必须要求query_tensor是bool，Long或binary且query_tensor的每一个值都小于target_tensor dim0的数量\n",
    "\n",
    "类似于embedding index的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/esmfold/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json,time\n",
    "import customize_data\n",
    "import utils\n",
    "import numpy as np\n",
    "import esm.esmfold.v1.esmfold as ESM\n",
    "import modules \n",
    "import os\n",
    "from openfold.utils.rigid_utils import Rigid\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK token:107,too long:0\n",
      "bad seq:107\n",
      "too long:0\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "data_file = \"data/chain_set.jsonl\"\n",
    "dataset = customize_data.StructureDataset(data_file)\n",
    "print(f\"bad seq:{dataset.discard['bad_chars']}\\ntoo long:{dataset.discard['too_long']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "dataLoader = customize_data.StructureDataloader(dataset,4,num_workers=1)\n",
    "batch_example = next(iter(dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data file and initialize the esm-inpainting class\n",
    "model_path = \"/root/.cache/torch/hub/checkpoints/esmfold_3B_v1.pt\"\n",
    "model_data = torch.load(str(model_path), map_location=\"cuda:0\") #读取一个pickle文件为一个dict\n",
    "cfg = model_data[\"cfg\"][\"model\"]\n",
    "model = modules.esm_inpaint(cfg) # make an instance\n",
    "model_state = model_data[\"model\"]\n",
    "model.esmfold.load_state_dict(model_state, strict=False)\n",
    "model.esmfold.set_chunk_size(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "# 11735MiB / 32510MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': 'torch.Size([2, 330, 4, 3])_torch.float32',\n",
       " 'seq': 'torch.Size([2, 330])_torch.int64',\n",
       " 'mask_coord': 'torch.Size([2, 330, 4, 3])_torch.float32',\n",
       " 'mask_seq': 'torch.Size([2, 330])_torch.int64',\n",
       " 'bert_mask_fraction': 'torch.Size([2, 1])_torch.float32',\n",
       " 'bert_mask': 'torch.Size([2, 330])_torch.bool',\n",
       " 'padding_mask': 'torch.Size([2, 330])_torch.float32'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.recur_print(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in batch_example.keys():\n",
    "    batch_example[key] = batch_example[key].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "Recycling:0-----------\n",
      "Folding Trunk\n",
      "Structure Module\n",
      "Recycling Output\n",
      "Recycling:1-----------\n",
      "Folding Trunk\n",
      "Structure Module\n",
      "Recycling Output\n",
      "Recycling:2-----------\n",
      "Folding Trunk\n",
      "Structure Module\n",
      "Recycling Output\n",
      "Recycling:3-----------\n",
      "Folding Trunk\n",
      "Structure Module\n",
      "Recycling Output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ESM-Inpainting/esm_inpaint/modules.py:106: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output_seq = F.log_softmax(structure['lm_logits'])\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "output = model(coord=batch_example['mask_coord'],mask=(batch_example['padding_mask']).to(torch.float32),S=batch_example['mask_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 330, 4, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_example['coord'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq Loss\n",
    "\n",
    "cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9019, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = batch_example['seq']\n",
    "log_pred = output['log_softmax_aa']\n",
    "padding_mask = batch_example['padding_mask']\n",
    "_, loss_av_smoothed = utils.loss_nll(seq, log_pred, padding_mask)\n",
    "loss_av_smoothed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone Frame Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,L = output['positions'].shape[:2]\n",
    "pred_position = output['positions'].reshape(B, -1, 3)\n",
    "target_position = batch_example['coord'][:,:,:3,:].reshape(B, -1 ,3)\n",
    "position_mask = torch.ones_like(target_position[...,0])\n",
    "# utils.compute_fape(Rigid.from_tensor_7(output['pred_frames']),output['target_frames'],batch_example['padding_mask'],pred_position,target_position,position_mask,1)\n",
    "\n",
    "fape_loss = torch.sum(utils.compute_fape(output['pred_frames'],output['target_frames'],batch_example['padding_mask'],pred_position,target_position,position_mask,10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0.1*fape_loss+loss_av_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59ac7c7a591b3b1c7843a2ac3c1da3475c2ffdc4684f266052d1e649f1d10700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
